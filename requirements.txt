packaging
torch==2.3.1  # error with flash-attn if use 2.3
#jupyter
torchvision
torchaudio
transformers
auto-gptq
ninja
accelerate
optimum
pydantic
uvicorn
#streamlit
sentence-transformers
lm-format-enforcer
openai
huggingface-hub
chardet
bitsandbytes
fastapi
sse_starlette
#pyautogen
prometheus_client
#aphrodite-gallama
langchain
#langgraph
#langchain-openai
#langchainhub
langchain-community
sentencepiece
lxml
qdrant-client
llama-cpp-python
#flash-attn
infinity-emb
colorama
pygments
anthropic
#mistralai
#flash-attn
#pygraphviz
#python -m pip install --pre --extra-index-url https://pypi.nvidia.com optimum-nvidia

